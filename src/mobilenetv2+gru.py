# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jNiRQwBLAx8Sh5v8FxJ6qZvs28unYb2h
"""

from google.colab import drive
drive.mount('/content/drive')

!cp -r "/content/drive/MyDrive/deepfake_checkpoints_24f_224" /content/
!cp "/content/drive/MyDrive/deepfake_data.zip" /content/

!ls /content/

!unzip -q /content/deepfake_data.zip -d /content/local_deepfake_data

INPUT_SHAPE = (24, 224, 224, 3)
N_FRAMES = 24
CHECKPOINT_PATH = '/content/deepfake_checkpoints_24f_224/weights_epoch_{epoch:03d}.weights.h5'
LAST_EPOCH_FILE = '/content/deepfake_checkpoints_24f_224/last_epoch_24f_224.txt'
stage1_epochs = 5
TOTAL_EPOCHS = 20

# ============================================================
# Environment + Path Configuration (for friend's Colab)
# ============================================================

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.mixed_precision import set_global_policy

try:
    set_global_policy("mixed_float16")
    print("‚úÖ Mixed precision set to 'mixed_float16'.")
except Exception as e:
    print(f"‚ö†Ô∏è Mixed precision unavailable: {e}")

# --- Core config ---
N_FRAMES = 24
BATCH_SIZE = 1
IMG_SIZE = (224, 224)
TOTAL_EPOCHS = 20
STAGE1_EPOCHS = 5
CLASSIFICATION_THRESHOLD = 0.5
INPUT_SHAPE = (IMG_SIZE[0], IMG_SIZE[1], 3)

# --- Friend's Colab paths ---
BASE_PATH = "/content/drive/MyDrive"  # friend's Drive (where you shared the files)
LOCAL_DATA_DIR = "/content/local_deepfake_data"  # extracted dataset folder
CHECKPOINT_DIR = "/content/deepfake_checkpoints_24f_224"  # copied from shared folder

# --- Files and checkpoints ---
LAST_EPOCH_FILE = os.path.join(CHECKPOINT_DIR, "last_epoch_24f_224.txt")
CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, "weights_epoch_{epoch:03d}.weights.h5")
BEST_WEIGHTS_PATH = os.path.join(CHECKPOINT_DIR, "best.weights.h5")
TEST_VIDEO_PATH = os.path.join(BASE_PATH, "test_videos", "sample_video.mp4")

print("‚úÖ Paths configured.")
print(f"LOCAL_DATA_DIR: {LOCAL_DATA_DIR}")
print(f"CHECKPOINT_DIR: {CHECKPOINT_DIR}")

# ============================================================
# Build Model (exactly as used in your first 7 epochs)
# ============================================================

from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    GRU, Dense, TimeDistributed, Input,
    GlobalAveragePooling2D, Dropout
)
from tensorflow.keras.applications import MobileNetV2

def build_model(input_shape, n_frames):
    """
    Builds the MobileNetV2 + GRU hybrid model for deepfake detection.
    Matches exactly the architecture used during the first 7 epochs.
    """
    # Base CNN for spatial feature extraction
    base = MobileNetV2(
        weights="imagenet",
        include_top=False,
        input_shape=input_shape,
        alpha=0.75
    )
    x = GlobalAveragePooling2D()(base.output)
    feature_extractor = Model(base.input, x)

    # Temporal model for sequence of frames
    seq_input = Input(shape=(n_frames, *input_shape))
    encoded = TimeDistributed(feature_extractor)(seq_input)
    gru = GRU(32)(encoded)
    d = Dropout(0.5)(gru)
    x = Dense(32, activation="relu")(d)
    x = Dropout(0.5)(x)
    out = Dense(1, activation="sigmoid", dtype="float32")(x)

    model = Model(seq_input, out)
    return model, base

# ============================================================
# AFTER UNZIPPING: Resume-safe 2-stage training on friend's Colab
# ============================================================

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint
from tensorflow.keras.utils import Sequence
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input

# ============================================================
# 1Ô∏è‚É£ Data Generator (exact same version used during first 7 epochs)
# ============================================================

class VideoSequenceGenerator(Sequence):
    def __init__(self, metadata, batch_size, n_frames, img_size, data_path, shuffle=True):
        self.metadata = metadata
        self.batch_size = batch_size
        self.n_frames = n_frames
        self.img_size = img_size
        self.data_path = data_path
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        return int(np.floor(len(self.metadata) / self.batch_size))

    def on_epoch_end(self):
        self.indexes = np.arange(len(self.metadata))
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def __getitem__(self, index):
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        batch_meta = [self.metadata[k] for k in indexes]
        X = np.empty((self.batch_size, self.n_frames, *self.img_size, 3), dtype=np.float32)
        y = np.empty((self.batch_size, 1), dtype=np.float32)
        for i, (vid_rel, label) in enumerate(batch_meta):
            abs_path = os.path.join(self.data_path, vid_rel)
            X[i,], y[i,] = self._load_sample(abs_path, label)
        return X, y

    def _load_sample(self, path, label):
        try:
            frames = np.load(path, allow_pickle=False)
            frames = preprocess_input(frames.astype(np.float32))
            y_data = np.array([float(label)], dtype=np.float32)
            return frames, y_data
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading {path}: {e}")
            X_fail = np.zeros((self.n_frames, *self.img_size, 3), dtype=np.float32)
            y_fail = np.array([0.0], dtype=np.float32)
            return X_fail, y_fail


# ============================================================
# 2Ô∏è‚É£ Main Training Function (resume-safe, 2-stage)
# ============================================================

def train_after_unzip():
    # 1) Load metadata
    train_meta = np.load(os.path.join(LOCAL_DATA_DIR, "train_metadata.npy"), allow_pickle=True)
    val_meta   = np.load(os.path.join(LOCAL_DATA_DIR, "val_metadata.npy"), allow_pickle=True)
    print(f"üìÅ Train samples: {len(train_meta)}, Val samples: {len(val_meta)}")

    # 2) Create generators
    train_gen = VideoSequenceGenerator(train_meta, BATCH_SIZE, N_FRAMES, IMG_SIZE, LOCAL_DATA_DIR, shuffle=True)
    val_gen   = VideoSequenceGenerator(val_meta,   BATCH_SIZE, N_FRAMES, IMG_SIZE, LOCAL_DATA_DIR, shuffle=False)

    # 3) Build model (uses your original architecture)
    model, base = build_model(INPUT_SHAPE, N_FRAMES)
    model.summary()

    # 4) Check for existing progress
    initial_epoch = 0
    if os.path.exists(LAST_EPOCH_FILE):
        try:
            with open(LAST_EPOCH_FILE, "r") as f:
                txt = f.read().strip()
            if txt.isdigit():
                initial_epoch = int(txt)
        except Exception as e:
            print(f"‚ö†Ô∏è Could not parse LAST_EPOCH_FILE: {e}")
            initial_epoch = 0

    # 5) Load existing weights safely
    if initial_epoch > 0:
        resume_path = CHECKPOINT_PATH.format(epoch=initial_epoch)
        if os.path.exists(resume_path):
            try:
                model.load_weights(resume_path)
                print(f"‚úÖ Loaded weights from {resume_path}")
            except Exception as e:
                print(f"‚ö†Ô∏è Strict load failed: {e}")
                try:
                    model.load_weights(resume_path, weights_only=True)
                    print("‚úÖ Loaded matching layers only (partial load).")
                except Exception as e2:
                    print(f"‚ùå Partial load failed: {e2}. Starting from scratch.")
                    initial_epoch = 0
        else:
            print("‚ö†Ô∏è No weight file found for resume. Starting fresh.")
            initial_epoch = 0
    else:
        print("üÜï Starting training from epoch 1.")

    # ============================================================
    # 6Ô∏è‚É£ Define Callbacks
    # ============================================================

    class EpochSaverCallback(Callback):
        def on_epoch_end(self, epoch, logs=None):
            e = epoch + 1
            weight_path = CHECKPOINT_PATH.format(epoch=e)
            self.model.save_weights(weight_path)
            with open(LAST_EPOCH_FILE, "w") as f:
                f.write(str(e))
            print(f"\nüíæ Saved checkpoint {weight_path} (epoch {e})")

    early_stop = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
    ckpt_best = ModelCheckpoint(
        BEST_WEIGHTS_PATH, monitor="val_loss", mode="min",
        save_best_only=True, save_weights_only=True, verbose=1
    )

    callbacks = [EpochSaverCallback(), early_stop, ckpt_best]

    # ============================================================
    # 7Ô∏è‚É£ Stage 1 ‚Äì Warm-Up
    # ============================================================

    last_completed_epoch = initial_epoch

    if initial_epoch < STAGE1_EPOCHS:
        print(f"\n--- Stage 1: Freeze MobileNetV2 (epochs {initial_epoch + 1} ‚Üí {STAGE1_EPOCHS}) ---")
        for l in base.layers:
            l.trainable = False

        model.compile(
            optimizer=tf.keras.optimizers.Adam(1e-3),
            loss="binary_crossentropy",
            metrics=["accuracy"]
        )

        hist1 = model.fit(
            train_gen,
            validation_data=val_gen,
            epochs=STAGE1_EPOCHS,
            initial_epoch=initial_epoch,
            callbacks=callbacks
        )

        if hasattr(hist1, "epoch") and len(hist1.epoch) > 0:
            last_completed_epoch = hist1.epoch[-1] + 1
        else:
            last_completed_epoch = initial_epoch
        initial_epoch = last_completed_epoch
        print(f"‚úÖ Stage 1 completed. Now resuming from epoch {initial_epoch}.")

    # ============================================================
    # 8Ô∏è‚É£ Stage 2 ‚Äì Fine-Tuning
    # ============================================================

    if initial_epoch < TOTAL_EPOCHS:
        print(f"\n--- Stage 2: Fine-tune MobileNetV2 (epochs {initial_epoch + 1} ‚Üí {TOTAL_EPOCHS}) ---")
        for l in base.layers:
            l.trainable = True

        model.compile(
            optimizer=tf.keras.optimizers.Adam(1e-5),
            loss="binary_crossentropy",
            metrics=["accuracy"]
        )

        hist2 = model.fit(
            train_gen,
            validation_data=val_gen,
            epochs=TOTAL_EPOCHS,
            initial_epoch=initial_epoch,
            callbacks=callbacks
        )

        if hasattr(hist2, "epoch") and len(hist2.epoch) > 0:
            last_completed_epoch = hist2.epoch[-1] + 1
        else:
            last_completed_epoch = initial_epoch
        print(f"‚úÖ Stage 2 complete. Trained up to epoch {last_completed_epoch}.")
    else:
        last_completed_epoch = initial_epoch

    # ============================================================
    # 9Ô∏è‚É£ Optional: Run test prediction
    # ============================================================

    if last_completed_epoch > 0:
        try:
            predict_video(model, last_completed_epoch)
        except Exception as e:
            print(f"‚ö†Ô∏è Prediction skipped: {e}")


# ============================================================
# Run Training
# ============================================================

train_after_unzip()



from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import os, zipfile

BASE_PATH = "/content/drive/MyDrive"
ZIP_PATH = os.path.join(BASE_PATH, "deepfake_data.zip")
EXTRACT_DIR = "/content/local_deepfake_data"

# Extract only if not already done
if not os.path.exists(os.path.join(EXTRACT_DIR, "train_metadata.npy")):
    print(f"üì¶ Extracting {ZIP_PATH} ...")
    os.makedirs(EXTRACT_DIR, exist_ok=True)
    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
        zip_ref.extractall(EXTRACT_DIR)
    print("‚úÖ Extraction complete.")
else:
    print("‚úÖ Dataset already extracted.")

import os

print("Files in local dataset folder:")
!ls -lh /content/local_deepfake_data | grep metadata

# ============================================================
# DeepFake Detection - Efficient Validation Evaluation
# ============================================================

import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import (
    confusion_matrix, roc_curve, roc_auc_score,
    classification_report, accuracy_score
)
import seaborn as sns
from tqdm import tqdm
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    GRU, Dense, TimeDistributed, Input,
    GlobalAveragePooling2D, Dropout
)
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from tensorflow.keras.utils import Sequence

# ============================================================
# Configuration
# ============================================================

N_FRAMES = 24
IMG_SIZE = (224, 224)
INPUT_SHAPE = (IMG_SIZE[0], IMG_SIZE[1], 3)
BATCH_SIZE = 1

DATA_DIR = "/content/local_deepfake_data"
VAL_META_PATH = os.path.join(DATA_DIR, "val_metadata.npy")
CHECKPOINT_PATH = "/content/drive/MyDrive/deepfake_checkpoints_24f_224/weights_epoch_012.weights.h5"

# ============================================================
# Data Generator
# ============================================================

class VideoSequenceGenerator(Sequence):
    def __init__(self, metadata, batch_size, n_frames, img_size, data_path):
        self.metadata = metadata
        self.batch_size = batch_size
        self.n_frames = n_frames
        self.img_size = img_size
        self.data_path = data_path

    def __len__(self):
        return int(np.floor(len(self.metadata) / self.batch_size))

    def __getitem__(self, index):
        batch_meta = self.metadata[index*self.batch_size:(index+1)*self.batch_size]
        X = np.empty((self.batch_size, self.n_frames, *self.img_size, 3), dtype=np.float32)
        y = np.empty((self.batch_size, 1), dtype=np.float32)
        for i, (vid_rel, label) in enumerate(batch_meta):
            path = os.path.join(self.data_path, vid_rel)
            frames = np.load(path, allow_pickle=False)
            frames = preprocess_input(frames.astype(np.float32))
            X[i] = frames
            y[i] = float(label)
        return X, y

# ============================================================
# Model Definition
# ============================================================

def build_model(input_shape, n_frames):
    base = MobileNetV2(weights="imagenet", include_top=False, input_shape=input_shape, alpha=0.75)
    x = GlobalAveragePooling2D()(base.output)
    feature_extractor = Model(base.input, x)

    seq_input = Input(shape=(n_frames, *input_shape))
    encoded = TimeDistributed(feature_extractor)(seq_input)
    gru = GRU(32)(encoded)
    d = Dropout(0.5)(gru)
    x = Dense(32, activation="relu")(d)
    x = Dropout(0.5)(x)
    out = Dense(1, activation="sigmoid", dtype="float32")(x)

    model = Model(seq_input, out)
    return model

# ============================================================
# Load Data + Model
# ============================================================

val_meta = np.load(VAL_META_PATH, allow_pickle=True)
val_gen = VideoSequenceGenerator(val_meta, BATCH_SIZE, N_FRAMES, IMG_SIZE, DATA_DIR)

model = build_model(INPUT_SHAPE, N_FRAMES)
model.load_weights(CHECKPOINT_PATH)
print(f"‚úÖ Loaded weights from {CHECKPOINT_PATH}")

# ============================================================
# Efficient Batched Prediction (with progress bar)
# ============================================================

print("\nüîç Running validation predictions...")

y_true, y_pred_prob = [], []
for i in tqdm(range(len(val_gen)), desc="Evaluating videos"):
    X, y = val_gen[i]
    preds = model.predict(X, verbose=0)
    y_true.extend(y.flatten())
    y_pred_prob.extend(preds.flatten())

y_true = np.array(y_true)
y_pred_prob = np.array(y_pred_prob)
y_pred = (y_pred_prob >= 0.5).astype(int)

# ============================================================
# Compute Metrics
# ============================================================

acc = accuracy_score(y_true, y_pred)
auc = roc_auc_score(y_true, y_pred_prob)
report = classification_report(y_true, y_pred, target_names=["Real", "Fake"], digits=4)
cm = confusion_matrix(y_true, y_pred)

print("\n================= Evaluation Summary =================")
print(f"‚úÖ Accuracy: {acc*100:.2f}%")
print(f"‚úÖ ROC-AUC: {auc:.4f}")
print("\nClassification Report:\n", report)
print("Confusion Matrix:\n", cm)
print("======================================================")

# ============================================================
# Visualization: Confusion Matrix & ROC
# ============================================================

plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Pred:Real','Pred:Fake'],
            yticklabels=['True:Real','True:Fake'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

fpr, tpr, _ = roc_curve(y_true, y_pred_prob)
plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {auc:.4f}")
plt.plot([0,1],[0,1],'--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

print("\nüìä Final Metrics Summary:")
print(f"Accuracy  : {acc*100:.2f}%")
print(f"ROC-AUC   : {auc:.4f}")
print("‚úÖ Evaluation completed successfully!")